{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Set global random seed for numpy and sklearn for reproducibility.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>city_code</th>\n",
       "      <th>product</th>\n",
       "      <th>destinated_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>0</td>\n",
       "      <td>960b4f2c94a2fb2c</td>\n",
       "      <td>Temporary-other</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>0</td>\n",
       "      <td>746cc42bfb8f6b62</td>\n",
       "      <td>Temporary-other</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>0</td>\n",
       "      <td>6cce2bf873870afc</td>\n",
       "      <td>Temporary-other</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>0</td>\n",
       "      <td>4de42e351006a2ae</td>\n",
       "      <td>Temporary-other</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>0</td>\n",
       "      <td>5b6072f8f6d37acc</td>\n",
       "      <td>Temporary-other</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73739</th>\n",
       "      <td>28</td>\n",
       "      <td>74d75dde6dc4a5ec</td>\n",
       "      <td>Livestock</td>\n",
       "      <td>150866.5644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73740</th>\n",
       "      <td>29</td>\n",
       "      <td>74d75dde6dc4a5ec</td>\n",
       "      <td>Livestock</td>\n",
       "      <td>152446.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73741</th>\n",
       "      <td>30</td>\n",
       "      <td>74d75dde6dc4a5ec</td>\n",
       "      <td>Livestock</td>\n",
       "      <td>154681.0205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73742</th>\n",
       "      <td>31</td>\n",
       "      <td>74d75dde6dc4a5ec</td>\n",
       "      <td>Livestock</td>\n",
       "      <td>164706.2778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73743</th>\n",
       "      <td>32</td>\n",
       "      <td>74d75dde6dc4a5ec</td>\n",
       "      <td>Livestock</td>\n",
       "      <td>165220.8728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56496 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year         city_code          product  destinated_area\n",
       "1573      0  960b4f2c94a2fb2c  Temporary-other           0.0000\n",
       "1574      0  746cc42bfb8f6b62  Temporary-other           0.0000\n",
       "1575      0  6cce2bf873870afc  Temporary-other           0.0000\n",
       "1576      0  4de42e351006a2ae  Temporary-other           0.0000\n",
       "1577      0  5b6072f8f6d37acc  Temporary-other           0.0000\n",
       "...     ...               ...              ...              ...\n",
       "73739    28  74d75dde6dc4a5ec        Livestock      150866.5644\n",
       "73740    29  74d75dde6dc4a5ec        Livestock      152446.3960\n",
       "73741    30  74d75dde6dc4a5ec        Livestock      154681.0205\n",
       "73742    31  74d75dde6dc4a5ec        Livestock      164706.2778\n",
       "73743    32  74d75dde6dc4a5ec        Livestock      165220.8728\n",
       "\n",
       "[56496 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important to use utf-8, as 'açaí' will need a better encoding than default ascii\n",
    "with open('historical-database.csv', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "data = data.split('\\n')\n",
    "# Separating columns manually\n",
    "columns = data[0]\n",
    "data = data[1:-1]\n",
    "# Remove ending comma, then split by semi-colon\n",
    "columns = columns[:-1].split(';')\n",
    "\n",
    "# Transform dates\n",
    "def date_handler(date: pd.Timestamp):\n",
    "    return date.year - 1985\n",
    "\n",
    "# Retransform dates back to pd.Timestamp\n",
    "def date_reverse(date: int) -> pd.Timestamp:\n",
    "    return pd.Timestamp(date + 1985, 1, 1)\n",
    "\n",
    "# Simple example to check that date encoding is working\n",
    "# sample_date = pd.Timestamp(2015, 1, 1)\n",
    "# cvt_date = date_handler(sample_date)\n",
    "# print(f'{sample_date=}\\n{cvt_date=}\\n{date_reverse(cvt_date)=}')\n",
    "\n",
    "def process_line(line: str) -> list:\n",
    "    entries = line.split(';')\n",
    "    if ',' in entries[-1] and len(entries[-1])>1:\n",
    "        entries[-1] = float(entries[-1].replace(',','.',-1))\n",
    "    elif len(entries[-1]) <= 1:\n",
    "        entries[-1] = None\n",
    "    else:\n",
    "        entries[-1] = float(entries[-1])\n",
    "    # Convert to python-friendly date format.\n",
    "    entries[0] = datetime.datetime.strptime(entries[0],\"%d/%m/%Y\")\n",
    "    return entries\n",
    "data = list(map(process_line, data))\n",
    "df = pd.DataFrame(data=data, columns=columns)\n",
    "df['year'] = df['year'].apply(date_handler)\n",
    "df = df.drop(df[df['year']<0].index, axis = 0)\n",
    "type_dict = [(val, tip) for tip, val in df[['product_type', 'product']].value_counts().index.to_list() if val != \"Others\"]\n",
    "type_dict.append(('Permanent-other', 'permanent'))\n",
    "type_dict.append(('Temporary-other', 'temporary'))\n",
    "type_dict = {prod: tip for prod, tip in type_dict}\n",
    "df.loc[(df['product_type'] == \"permanent\") & (df['product'] == 'Others'), 'product'] = 'Permanent-other'\n",
    "df.loc[(df['product_type'] == \"temporary\") & (df['product'] == 'Others'), 'product'] = 'Temporary-other'\n",
    "df = df.drop('product_type', axis=1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>city_code</th>\n",
       "      <th>product</th>\n",
       "      <th>destinated_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>960b4f2c94a2fb2c</td>\n",
       "      <td>Temporary-other</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>746cc42bfb8f6b62</td>\n",
       "      <td>Temporary-other</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>6cce2bf873870afc</td>\n",
       "      <td>Temporary-other</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4de42e351006a2ae</td>\n",
       "      <td>Temporary-other</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5b6072f8f6d37acc</td>\n",
       "      <td>Temporary-other</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56491</th>\n",
       "      <td>28</td>\n",
       "      <td>74d75dde6dc4a5ec</td>\n",
       "      <td>Livestock</td>\n",
       "      <td>150866.5644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56492</th>\n",
       "      <td>29</td>\n",
       "      <td>74d75dde6dc4a5ec</td>\n",
       "      <td>Livestock</td>\n",
       "      <td>152446.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56493</th>\n",
       "      <td>30</td>\n",
       "      <td>74d75dde6dc4a5ec</td>\n",
       "      <td>Livestock</td>\n",
       "      <td>154681.0205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56494</th>\n",
       "      <td>31</td>\n",
       "      <td>74d75dde6dc4a5ec</td>\n",
       "      <td>Livestock</td>\n",
       "      <td>164706.2778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56495</th>\n",
       "      <td>32</td>\n",
       "      <td>74d75dde6dc4a5ec</td>\n",
       "      <td>Livestock</td>\n",
       "      <td>165220.8728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56496 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year         city_code          product  destinated_area\n",
       "0         0  960b4f2c94a2fb2c  Temporary-other           0.0000\n",
       "1         0  746cc42bfb8f6b62  Temporary-other           0.0000\n",
       "2         0  6cce2bf873870afc  Temporary-other           0.0000\n",
       "3         0  4de42e351006a2ae  Temporary-other           0.0000\n",
       "4         0  5b6072f8f6d37acc  Temporary-other           0.0000\n",
       "...     ...               ...              ...              ...\n",
       "56491    28  74d75dde6dc4a5ec        Livestock      150866.5644\n",
       "56492    29  74d75dde6dc4a5ec        Livestock      152446.3960\n",
       "56493    30  74d75dde6dc4a5ec        Livestock      154681.0205\n",
       "56494    31  74d75dde6dc4a5ec        Livestock      164706.2778\n",
       "56495    32  74d75dde6dc4a5ec        Livestock      165220.8728\n",
       "\n",
       "[56496 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRODUCTION = False\n",
    "# Insert two next years into df for final predictions\n",
    "\n",
    "UNIQUE_PRODUCT_LIST = list(set(df['product'].value_counts().index))\n",
    "UNIQUE_CITY_CODES = list(set(df['city_code'].value_counts().index))\n",
    "FINAL_YEAR = df['year'].max()\n",
    "\n",
    "if PRODUCTION:\n",
    "    for city in UNIQUE_CITY_CODES:\n",
    "        for prod in UNIQUE_PRODUCT_LIST:\n",
    "            sm_df = pd.DataFrame(data=[[FINAL_YEAR+1, city, prod, np.nan], [FINAL_YEAR+2, city, prod, np.nan]], columns=df.columns)\n",
    "            df = pd.concat([df, sm_df])  \n",
    "    FINAL_YEAR += 2      \n",
    "        \n",
    "df = df.reset_index().drop('index',axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse data for missing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                   0\n",
       "city_code              0\n",
       "product                0\n",
       "destinated_area    24933\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there a lot of missing values. This is very important to note, since in some cases, a city might not have any records for a certain product, for example, city '8e0eb10270d768f8' might never have had any records regarding the destined area for planting Sorghum. This means that accurately predicting this value is not possible. The best one may do, is to simply assume that it has always been zero, and as such, will remain zero. There is another case that is quite common, which is records starting at some point after the beginning of the records, and occasionaly faltering. In this case, where possible, we choose to interpolate the data to fill gaps, and assume it was 0 until the beginning of known records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standard params for models in dictionaries\n",
    "default_elastic_net_params = {'alpha':1, 'random_state':42, 'max_iter':10000000, 'tol':1e-1, 'fit_intercept':False}\n",
    "# Degree is 15 to allow for possible capture of high speed change, while also beign kept in check by regularization.\n",
    "default_polynomial_feat_params = {'degree':15, 'include_bias':False}\n",
    "\n",
    "# Separating df by city\n",
    "def df_for_city(city_code: str) -> pd.DataFrame:\n",
    "    global df\n",
    "    return df[df['city_code'] == city_code]\n",
    "\n",
    "def show_crops(dataframe: pd.DataFrame):\n",
    "    '''\n",
    "    Show every crop's progression over every year on the database.\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(16,20))\n",
    "    axList = []\n",
    "    for i, prod in enumerate(list(dataframe['product'].value_counts().index)):\n",
    "        # print(i, prod)\n",
    "        sub_df = dataframe[dataframe['product'] == prod][['destinated_area','year']].sort_values('year', ascending=True)\n",
    "        axList.append(fig.add_subplot(4,3,i+1))\n",
    "        axList[i].set_title(prod)\n",
    "        axList[i].plot(sub_df['year'], sub_df['destinated_area'])\n",
    "    fig.show()\n",
    "\n",
    "def construct_city_db(curr_city_code: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Given a city code, extract all lines for that city\n",
    "    '''\n",
    "    curr_df = df_for_city(curr_city_code).drop('city_code', axis=1)\n",
    "    return curr_df\n",
    "\n",
    "def treat_missing(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Given a dataframe for a single city, treat all missing values as best as possible;\n",
    "    '''\n",
    "    dfs=[]\n",
    "    for i, prod in enumerate(list(dataframe['product'].value_counts().index)):\n",
    "            # Extract relevant columns for this product\n",
    "            sub_df = dataframe[dataframe['product'] == prod].sort_values('year')\n",
    "            # Determine how many points are missing\n",
    "            null_values = sub_df['destinated_area'].isna().sum()\n",
    "            # If all points are missing, assume always 0\n",
    "            if null_values == len(list(sub_df.index)):\n",
    "                    sub_df.loc[sub_df['product'] == prod,'destinated_area'] = 0\n",
    "                    # print(prod, null_values, sub_df)\n",
    "            # If you have some data, interpolate to determine missing points in between\n",
    "            if null_values != 0:\n",
    "                    # Interpolate where possible\n",
    "                    sub_df = sub_df.interpolate(axis=0)\n",
    "                    # Fill with 0 where can't extrapolate\n",
    "                    sub_df = sub_df.fillna(0.0)\n",
    "            dfs.append(sub_df)\n",
    "    return pd.concat(dfs)   \n",
    "            \n",
    "# Now that data is somewhat smoothed and cleaned up\n",
    "# We begin splitting and training\n",
    "\n",
    "class ConstantPredictor():\n",
    "    '''\n",
    "    Simple predictor class to always return a constant value (mostly to handle cases where we have no data)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.out_shape=(1,1)\n",
    "    \n",
    "    def fit(self, x=[], y=[]):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.ones(X.shape[0])*self.k\n",
    "\n",
    "def break_by_city(dataframe: pd.DataFrame) -> list:\n",
    "    '''\n",
    "    Given a dataframe for a single city, separate into separate dataframes by product;\n",
    "    '''\n",
    "    dataframes={}\n",
    "    for i, prod in enumerate(list(dataframe['product'].value_counts().index)):\n",
    "            # Extract relevant columns for this product\n",
    "            sub_dataframe = dataframe[dataframe['product'] == prod].sort_values('year')\n",
    "            dataframes[prod] = (sub_dataframe.reset_index().drop(['index', 'product'], axis=1))\n",
    "    return dataframes\n",
    "\n",
    "def train_test_split(dataframe: pd.DataFrame) -> list:\n",
    "    '''\n",
    "    Function that takes the dataframe of a single product, in a single city, and separates the last two years as test data\n",
    "    '''\n",
    "    global FINAL_YEAR\n",
    "    TEST = dataframe[dataframe['year'] > FINAL_YEAR - 2]\n",
    "    TRAIN = dataframe[dataframe['year'] <= FINAL_YEAR - 2]\n",
    "    X_test = TEST[['year']]\n",
    "    y_test = TEST[['destinated_area']]\n",
    "    X_train = TRAIN[['year']]\n",
    "    y_train = TRAIN[['destinated_area']].diff(axis=0)\n",
    "    return [np.array(i) for i in [X_train, y_train.fillna(0), X_test, y_test]]\n",
    "\n",
    "def plot_regressions(dataframe: pd.DataFrame) -> None:\n",
    "    '''\n",
    "    Function to help visualize the quality of the regression models. Receives a dataframe for a single city\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(16,20))\n",
    "    axList = []\n",
    "    for i, p in enumerate(list(dataframe['product'].value_counts().index)):\n",
    "        p_dataframe = break_by_city(dataframe)[p]\n",
    "        # Homemade function to separate last 2 years as test for validation\n",
    "        X_train, y_train, X_test, y_test = train_test_split(p_dataframe)\n",
    "        # print(f'{X_train.shape=}\\n{X_test.shape=}\\n{y_train.shape=}\\n{y_test.shape=}')\n",
    "        \n",
    "        # Create polynomial features to capture nuances of data better\n",
    "        polfeat = PolynomialFeatures(**default_polynomial_feat_params)\n",
    "        X_train_poly = polfeat.fit_transform(X_train)\n",
    "\n",
    "        # If values are always zero, always predict 0.\n",
    "        if(np.abs(y_train).sum() < 1e-2):\n",
    "            reg = ConstantPredictor(0)\n",
    "        else:\n",
    "            # Use L1 and L2 regularization to prevent overfitting and only use features that actually help\n",
    "            reg = ElasticNet(**default_elastic_net_params)\n",
    "            reg.fit(X_train_poly, y_train)\n",
    "\n",
    "        axList.append(fig.add_subplot(4,3,i+1))\n",
    "        axList[i].set_title(p)\n",
    "        axList[i].plot(X_train, reg.predict(X_train_poly), c='r', label='Model')\n",
    "        axList[i].plot(X_train, y_train, label = 'Data')\n",
    "        axList[i].legend()\n",
    "    fig.show()\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def make_predictions(dataframe: pd.DataFrame):\n",
    "    '''\n",
    "    Given a dataframe of a city, creates, then applies a regressor to each product, then returns the prediction for the two years of validation.\n",
    "    '''\n",
    "    predictions = []\n",
    "    for i, p in enumerate(list(dataframe['product'].value_counts().index)):\n",
    "        p_dataframe = break_by_city(dataframe)[p]\n",
    "        # Homemade function to separate last 2 years as test for validation\n",
    "        X_train, y_train, X_test, y_test = train_test_split(p_dataframe)\n",
    "        # print(f'{X_train.shape=}\\n{X_test.shape=}\\n{y_train.shape=}\\n{y_test.shape=}')\n",
    "        # print(X_test.T)\n",
    "        # Create polynomial features to capture nuances of data better\n",
    "        polfeat = PolynomialFeatures(**default_polynomial_feat_params)\n",
    "        X_train_poly = polfeat.fit_transform(X_train)\n",
    "\n",
    "        # If values are always zero, always predict 0.\n",
    "        if(np.abs(y_train).sum() < 1e-2):\n",
    "            reg = ConstantPredictor(0)\n",
    "        else:\n",
    "            # Use L1 and L2 regularization to prevent overfitting and only use features that actually help\n",
    "            reg = ElasticNet(**default_elastic_net_params)\n",
    "            reg.fit(X_train_poly, y_train)\n",
    "        X_test_poly = polfeat.transform(X_test)\n",
    "        y_pred = reg.predict(X_test_poly)\n",
    "        # print(y_pred)\n",
    "        if PRODUCTION:\n",
    "            predictions.append([[int(t), p, float(r)] for r, t in zip(y_pred, X_test.T[0])])\n",
    "        else:\n",
    "            predictions.append([[t, r] for t, r in zip(y_pred,y_test[:,0])])\n",
    "    predictions = np.array(predictions)\n",
    "    if PRODUCTION:\n",
    "        # print(predictions)\n",
    "        return np.concatenate([predictions[:,0], predictions[:,1]])\n",
    "    return predictions\n",
    "\n",
    "def score_predictions(pred: list) -> int:\n",
    "    '''\n",
    "    Calculate the score (WMAPE) for the predictions given. Input is meant to be same format as 'make_predictions' output.\n",
    "    '''\n",
    "    def wmape(y):\n",
    "        y_pred, y_true = y\n",
    "        if y_true < 1e-2:\n",
    "            return abs(y_true-y_pred)\n",
    "        return abs(y_true-y_pred)/abs(y_true)\n",
    "    return sum(list(map(wmape, pred[:,0])))\n",
    "\n",
    "def transform_to_output_format(pred) -> pd.DataFrame:\n",
    "    city_code = list(pred.keys())[0]\n",
    "    dataframe = pd.DataFrame(data=list(pred.values())[0], columns=['year', 'product', 'predicted_area_change'])\n",
    "    dataframe['year'] = dataframe['year'].astype(np.int64)\n",
    "    dataframe['predicted_area_change'] = dataframe['predicted_area_change'].astype(np.float64)\n",
    "    base = df[(df['city_code'] == city_code)&(df['year'] == FINAL_YEAR - 2)].fillna(0).reset_index()\n",
    "    new_areas = []\n",
    "    for p in set(dataframe['product']):\n",
    "        area = list(base[base['product'] == p]['destinated_area'])[0]\n",
    "        area_2018 = list(dataframe[(dataframe['product'] == p) & (dataframe['year'] == FINAL_YEAR - 1)]['predicted_area_change'])[0]+area\n",
    "        area_2019 = list(dataframe[(dataframe['product'] == p) & (dataframe['year'] == FINAL_YEAR)]['predicted_area_change'])[0]+area_2018\n",
    "        if area_2018 < 0:\n",
    "            area_2018 = 0.0\n",
    "        if area_2019 < 0:\n",
    "            area_2019 = 0.0\n",
    "        new_areas.append([date_reverse(FINAL_YEAR - 1), city_code, type_dict[p], p, area_2018])\n",
    "        new_areas.append([date_reverse(FINAL_YEAR), city_code, type_dict[p], p, area_2019])\n",
    "    return pd.DataFrame(data=new_areas, columns=['year','city_code','product_type','product','destinated_area'])\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def make_score_for_city(city_code: str, show_graphs = False) -> int:\n",
    "    dataframe = construct_city_db(city_code)\n",
    "    dataframe = treat_missing(dataframe)\n",
    "    pred = make_predictions(dataframe)\n",
    "    if show_graphs:\n",
    "        plot_regressions(dataframe)\n",
    "    if PRODUCTION:\n",
    "        return transform_to_output_format({city_code: pred})\n",
    "    return score_predictions(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the training. This is done using an ElasticNet linear regression, with some basic feature engineering (adding polynomial features). This is useful, in case some of the progression haven't grown linearly with time, but instead quadratically, cubically, or really any polynomial up to a somewhat high degree (15 in this instance). It is worth noting however, ElasticNet enforces both L1 and L2 norms, which means, that the model will only use degree 15 if it is absolutely worth adding, seeing as these norms attempt to reduce the number of used features (which has several advantages, including the reduction of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cadud\\Repos\\Bain\\city_clustering_v2.ipynb Cell 11'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPool(\u001b[39m8\u001b[39m) \u001b[39mas\u001b[39;00m p:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000010?line=1'>2</a>\u001b[0m     a \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mmap(make_score_for_city, UNIQUE_CITY_CODES)\n",
      "File \u001b[1;32md:\\Python\\lib\\multiprocessing\\pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=358'>359</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=359'>360</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=360'>361</a>\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=361'>362</a>\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=362'>363</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=363'>364</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[1;32md:\\Python\\lib\\multiprocessing\\pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=768'>769</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=769'>770</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=770'>771</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32md:\\Python\\lib\\multiprocessing\\pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=122'>123</a>\u001b[0m job, i, func, args, kwds \u001b[39m=\u001b[39m task\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=123'>124</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=124'>125</a>\u001b[0m     result \u001b[39m=\u001b[39m (\u001b[39mTrue\u001b[39;00m, func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=125'>126</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=126'>127</a>\u001b[0m     \u001b[39mif\u001b[39;00m wrap_exception \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[1;32md:\\Python\\lib\\multiprocessing\\pool.py:48\u001b[0m, in \u001b[0;36mmapstar\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmapstar\u001b[39m(args):\n\u001b[1;32m---> <a href='file:///d%3A/Python/lib/multiprocessing/pool.py?line=47'>48</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(\u001b[39m*\u001b[39;49margs))\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\sklearn\\utils\\_testing.py:313\u001b[0m, in \u001b[0;36m_IgnoreWarnings.__call__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/_testing.py?line=310'>311</a>\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/_testing.py?line=311'>312</a>\u001b[0m     warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategory)\n\u001b[1;32m--> <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/_testing.py?line=312'>313</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32mc:\\Users\\cadud\\Repos\\Bain\\city_clustering_v2.ipynb Cell 9'\u001b[0m in \u001b[0;36mmake_score_for_city\u001b[1;34m(city_code, show_graphs)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000008?line=195'>196</a>\u001b[0m dataframe \u001b[39m=\u001b[39m construct_city_db(city_code)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000008?line=196'>197</a>\u001b[0m dataframe \u001b[39m=\u001b[39m treat_missing(dataframe)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000008?line=197'>198</a>\u001b[0m pred \u001b[39m=\u001b[39m make_predictions(dataframe)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000008?line=198'>199</a>\u001b[0m \u001b[39mif\u001b[39;00m show_graphs:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000008?line=199'>200</a>\u001b[0m     plot_regressions(dataframe)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\sklearn\\utils\\_testing.py:313\u001b[0m, in \u001b[0;36m_IgnoreWarnings.__call__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/_testing.py?line=310'>311</a>\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/_testing.py?line=311'>312</a>\u001b[0m     warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategory)\n\u001b[1;32m--> <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/_testing.py?line=312'>313</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32mc:\\Users\\cadud\\Repos\\Bain\\city_clustering_v2.ipynb Cell 9'\u001b[0m in \u001b[0;36mmake_predictions\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000008?line=146'>147</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000008?line=147'>148</a>\u001b[0m     \u001b[39m# Use L1 and L2 regularization to prevent overfitting and only use features that actually help\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000008?line=148'>149</a>\u001b[0m     reg \u001b[39m=\u001b[39m ElasticNet(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_elastic_net_params)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000008?line=149'>150</a>\u001b[0m     reg\u001b[39m.\u001b[39;49mfit(X_train_poly, y_train)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000008?line=150'>151</a>\u001b[0m X_test_poly \u001b[39m=\u001b[39m polfeat\u001b[39m.\u001b[39mtransform(X_test)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/cadud/Repos/Bain/city_clustering_v2.ipynb#ch0000008?line=151'>152</a>\u001b[0m y_pred \u001b[39m=\u001b[39m reg\u001b[39m.\u001b[39mpredict(X_test_poly)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:935\u001b[0m, in \u001b[0;36mElasticNet.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=932'>933</a>\u001b[0m \u001b[39mif\u001b[39;00m check_input:\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=933'>934</a>\u001b[0m     X_copied \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy_X \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept\n\u001b[1;32m--> <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=934'>935</a>\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=935'>936</a>\u001b[0m         X,\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=936'>937</a>\u001b[0m         y,\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=937'>938</a>\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=938'>939</a>\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mF\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=939'>940</a>\u001b[0m         dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=940'>941</a>\u001b[0m         copy\u001b[39m=\u001b[39;49mX_copied,\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=941'>942</a>\u001b[0m         multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=942'>943</a>\u001b[0m         y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=943'>944</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=944'>945</a>\u001b[0m     y \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=945'>946</a>\u001b[0m         y, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=946'>947</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/linear_model/_coordinate_descent.py?line=948'>949</a>\u001b[0m n_samples, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/base.py?line=578'>579</a>\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/base.py?line=579'>580</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/Python/lib/site-packages/sklearn/base.py?line=580'>581</a>\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/base.py?line=581'>582</a>\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/base.py?line=583'>584</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\sklearn\\utils\\validation.py:979\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=961'>962</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=963'>964</a>\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=964'>965</a>\u001b[0m     X,\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=965'>966</a>\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=975'>976</a>\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=976'>977</a>\u001b[0m )\n\u001b[1;32m--> <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=978'>979</a>\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39;49mmulti_output, y_numeric\u001b[39m=\u001b[39;49my_numeric)\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=980'>981</a>\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=982'>983</a>\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\sklearn\\utils\\validation.py:989\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=986'>987</a>\u001b[0m \u001b[39m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=987'>988</a>\u001b[0m \u001b[39mif\u001b[39;00m multi_output:\n\u001b[1;32m--> <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=988'>989</a>\u001b[0m     y \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=989'>990</a>\u001b[0m         y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, force_all_finite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=990'>991</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=991'>992</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=992'>993</a>\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\sklearn\\utils\\validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=793'>794</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=794'>795</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=795'>796</a>\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=796'>797</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=798'>799</a>\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=799'>800</a>\u001b[0m         _assert_all_finite(array, allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=801'>802</a>\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=802'>803</a>\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\sklearn\\utils\\validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=106'>107</a>\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=107'>108</a>\u001b[0m         allow_nan\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=108'>109</a>\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=109'>110</a>\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=110'>111</a>\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=111'>112</a>\u001b[0m     ):\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=112'>113</a>\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=113'>114</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=114'>115</a>\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=115'>116</a>\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=116'>117</a>\u001b[0m             )\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=117'>118</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=118'>119</a>\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Python/lib/site-packages/sklearn/utils/validation.py?line=119'>120</a>\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "with ThreadPool(8) as p:\n",
    "    a = p.map(make_score_for_city, UNIQUE_CITY_CODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = make_score_for_city(UNIQUE_CITY_CODES[5], show_graphs=True)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame([[a,b] for a, b in zip(a, UNIQUE_CITY_CODES)],columns=['WMAPE', 'city_code']).to_csv('./results/city_clustering_v2.csv')\n",
    "sum(a)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
