{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Set global random seed for numpy and sklearn for reproducibility.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important to use utf-8, as 'açaí' will need a better encoding than default ascii\n",
    "with open('historical-database.csv', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "data = data.split('\\n')\n",
    "# Separating columns manually\n",
    "columns = data[0]\n",
    "data = data[1:-1]\n",
    "# Remove ending comma, then split by semi-colon\n",
    "columns = columns[:-1].split(';')\n",
    "\n",
    "# Transform dates\n",
    "def date_handler(date: pd.Timestamp):\n",
    "    return date.year - 1985\n",
    "\n",
    "# Retransform dates back to pd.Timestamp\n",
    "def date_reverse(date: int) -> pd.Timestamp:\n",
    "    return pd.Timestamp(date + 1985, 1, 1)\n",
    "\n",
    "# Simple example to check that date encoding is working\n",
    "# sample_date = pd.Timestamp(2015, 1, 1)\n",
    "# cvt_date = date_handler(sample_date)\n",
    "# print(f'{sample_date=}\\n{cvt_date=}\\n{date_reverse(cvt_date)=}')\n",
    "\n",
    "def process_line(line: str) -> list:\n",
    "    entries = line.split(';')\n",
    "    if ',' in entries[-1] and len(entries[-1])>1:\n",
    "        entries[-1] = float(entries[-1].replace(',','.',-1))\n",
    "    elif len(entries[-1]) <= 1:\n",
    "        entries[-1] = None\n",
    "    else:\n",
    "        entries[-1] = float(entries[-1])\n",
    "    # Convert to python-friendly date format.\n",
    "    entries[0] = datetime.datetime.strptime(entries[0],\"%d/%m/%Y\")\n",
    "    return entries\n",
    "data = list(map(process_line, data))\n",
    "df = pd.DataFrame(data=data, columns=columns)\n",
    "df['year'] = df['year'].apply(date_handler)\n",
    "df = df.drop(df[df['year']<0].index, axis = 0)\n",
    "df.loc[(df['product_type'] == \"permanent\") & (df['product'] == 'Others'), 'product'] = 'Permanent-other'\n",
    "df.loc[(df['product_type'] == \"temporary\") & (df['product'] == 'Others'), 'product'] = 'Temporary-other'\n",
    "df = df.drop('product_type', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODUCTION = False\n",
    "\n",
    "# Insert two next years into df for final predictions\n",
    "\n",
    "UNIQUE_PRODUCT_LIST = list(set(df['product'].value_counts().index))\n",
    "UNIQUE_CITY_CODES = list(set(df['city_code'].value_counts().index))\n",
    "FINAL_YEAR = df['year'].max()\n",
    "\n",
    "if PRODUCTION:\n",
    "    for city in UNIQUE_CITY_CODES:\n",
    "        for prod in UNIQUE_PRODUCT_LIST:\n",
    "            sm_df = pd.DataFrame(data=[[FINAL_YEAR+1, city, prod, np.nan], [FINAL_YEAR+2, city, prod, np.nan]], columns=df.columns)\n",
    "            df = pd.concat([df, sm_df])        \n",
    "        \n",
    "df = df.reset_index().drop('index',axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse data for missing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there a lot of missing values. This is very important to note, since in some cases, a city might not have any records for a certain product, for example, city '8e0eb10270d768f8' might never have had any records regarding the destined area for planting Sorghum. This means that accurately predicting this value is not possible. The best one may do, is to simply assume that it has always been zero, and as such, will remain zero. There is another case that is quite common, which is records starting at some point after the beginning of the records, and occasionaly faltering. In this case, where possible, we choose to interpolate the data to fill gaps, and assume it was 0 until the beginning of known records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating df by city\n",
    "def df_for_city(city_code: str) -> pd.DataFrame:\n",
    "    global df\n",
    "    return df[df['city_code'] == city_code]\n",
    "\n",
    "def show_crops(dataframe: pd.DataFrame):\n",
    "    '''\n",
    "    Show every crop's progression over every year on the database.\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(16,20))\n",
    "    axList = []\n",
    "    for i, prod in enumerate(list(dataframe['product'].value_counts().index)):\n",
    "        # print(i, prod)\n",
    "        sub_df = dataframe[dataframe['product'] == prod][['destinated_area','year']].sort_values('year', ascending=True)\n",
    "        axList.append(fig.add_subplot(4,3,i+1))\n",
    "        axList[i].set_title(prod)\n",
    "        axList[i].plot(sub_df['year'], sub_df['destinated_area'])\n",
    "    fig.show()\n",
    "\n",
    "def construct_city_db(curr_city_code: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Given a city code, extract all lines for that city\n",
    "    '''\n",
    "    curr_df = df_for_city(curr_city_code).drop('city_code', axis=1)\n",
    "    return curr_df\n",
    "\n",
    "def treat_missing(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Given a dataframe for a single city, treat all missing values as best as possible;\n",
    "    '''\n",
    "    dfs=[]\n",
    "    for i, prod in enumerate(list(dataframe['product'].value_counts().index)):\n",
    "            # Extract relevant columns for this product\n",
    "            sub_df = dataframe[dataframe['product'] == prod].sort_values('year')\n",
    "            # Determine how many points are missing\n",
    "            null_values = sub_df['destinated_area'].isna().sum()\n",
    "            # If all points are missing, assume always 0\n",
    "            if null_values == len(list(sub_df.index)):\n",
    "                    sub_df.loc[sub_df['product'] == prod,'destinated_area'] = 0\n",
    "                    # print(prod, null_values, sub_df)\n",
    "            # If you have some data, interpolate to determine missing points in between\n",
    "            if null_values != 0:\n",
    "                    # Interpolate where possible\n",
    "                    sub_df = sub_df.interpolate(axis=0)\n",
    "                    # Fill with 0 where can't extrapolate\n",
    "                    sub_df = sub_df.fillna(0.0)\n",
    "            dfs.append(sub_df)\n",
    "    return pd.concat(dfs)   \n",
    "            \n",
    "# Now that data is somewhat smoothed and cleaned up\n",
    "# We begin splitting and training\n",
    "\n",
    "# Define standard params for models in dictionaries\n",
    "default_elastic_net_params = {'alpha':1, 'random_state':42, 'max_iter':1000000, 'tol':1e-1, 'fit_intercept':False}\n",
    "# Degree is 15 to allow for possible capture of high speed change, while also beign kept in check by regularization.\n",
    "default_polynomial_feat_params = {'degree':5, 'include_bias':True}\n",
    "\n",
    "class ConstantPredictor():\n",
    "    '''\n",
    "    Simple predictor class to always return a constant value (mostly to handle cases where we have no data)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.out_shape=(1,1)\n",
    "    \n",
    "    def fit(self, x=[], y=[]):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.ones(X.shape[0])*self.k\n",
    "\n",
    "def break_by_city(dataframe: pd.DataFrame) -> list:\n",
    "    '''\n",
    "    Given a dataframe for a single city, separate into separate dataframes by product;\n",
    "    '''\n",
    "    dataframes={}\n",
    "    for i, prod in enumerate(list(dataframe['product'].value_counts().index)):\n",
    "            # Extract relevant columns for this product\n",
    "            sub_dataframe = dataframe[dataframe['product'] == prod].sort_values('year')\n",
    "            dataframes[prod] = (sub_dataframe.reset_index().drop(['index', 'product'], axis=1))\n",
    "    return dataframes\n",
    "\n",
    "def train_test_split(dataframe: pd.DataFrame) -> list:\n",
    "    '''\n",
    "    Function that takes the dataframe of a single product, in a single city, and separates the last two years as test data\n",
    "    '''\n",
    "    global FINAL_YEAR\n",
    "    TEST = dataframe[dataframe['year'] >= FINAL_YEAR - 2]\n",
    "    TRAIN = dataframe[dataframe['year'] < FINAL_YEAR - 2]\n",
    "    X_test = TEST[['year']]\n",
    "    y_test = TEST[['destinated_area']]\n",
    "    X_train = TRAIN[['year']]\n",
    "    y_train = TRAIN[['destinated_area']].diff(axis=0).fillna(0)\n",
    "    return [np.array(i) for i in [X_train, y_train, X_test, y_test]]\n",
    "\n",
    "def plot_regressions(dataframe: pd.DataFrame) -> None:\n",
    "    '''\n",
    "    Function to help visualize the quality of the regression models. Receives a dataframe for a single city\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(16,20))\n",
    "    axList = []\n",
    "    for i, p in enumerate(list(dataframe['product'].value_counts().index)):\n",
    "        p_dataframe = break_by_city(dataframe)[p]\n",
    "        # Homemade function to separate last 2 years as test for validation\n",
    "        X_train, y_train, X_test, y_test = train_test_split(p_dataframe)\n",
    "        print(f'{X_train.shape=}\\n{X_test.shape=}\\n{y_train.shape=}\\n{y_test.shape=}')\n",
    "        \n",
    "        # Create polynomial features to capture nuances of data better\n",
    "        polfeat = PolynomialFeatures(**default_polynomial_feat_params)\n",
    "        X_train_poly = polfeat.fit_transform(X_train)\n",
    "\n",
    "        # If values are always zero, always predict 0.\n",
    "        if(np.abs(y_train).sum() < 1e-2):\n",
    "            reg = ConstantPredictor(0)\n",
    "        else:\n",
    "            # Use L1 and L2 regularization to prevent overfitting and only use features that actually help\n",
    "            reg = ElasticNet(**default_elastic_net_params)\n",
    "            reg.fit(X_train_poly, y_train)\n",
    "\n",
    "        axList.append(fig.add_subplot(4,3,i+1))\n",
    "        axList[i].set_title(p)\n",
    "        axList[i].plot(X_train, reg.predict(X_train_poly), c='r', label='Model')\n",
    "        axList[i].plot(X_train, y_train, label = 'Data')\n",
    "        axList[i].legend()\n",
    "    fig.show()\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def make_predictions(dataframe: pd.DataFrame):\n",
    "    '''\n",
    "    Given a dataframe of a city, creates, then applies a regressor to each product, then returns the prediction for the two years of validation.\n",
    "    '''\n",
    "    predictions = []\n",
    "    for i, p in enumerate(list(dataframe['product'].value_counts().index)):\n",
    "        p_dataframe = break_by_city(dataframe)[p]\n",
    "        # Homemade function to separate last 2 years as test for validation\n",
    "        X_train, y_train, X_test, y_test = train_test_split(p_dataframe)\n",
    "        # print(f'{X_train.shape=}\\n{X_test.shape=}\\n{y_train.shape=}\\n{y_test.shape=}')\n",
    "        \n",
    "        # Create polynomial features to capture nuances of data better\n",
    "        polfeat = PolynomialFeatures(**default_polynomial_feat_params)\n",
    "        X_train_poly = polfeat.fit_transform(X_train)\n",
    "\n",
    "        # If values are always zero, always predict 0.\n",
    "        if(np.abs(y_train).sum() < 1e-2):\n",
    "            reg = ConstantPredictor(0)\n",
    "        else:\n",
    "            # Use L1 and L2 regularization to prevent overfitting and only use features that actually help\n",
    "            reg = ElasticNet(**default_elastic_net_params)\n",
    "            reg.fit(X_train_poly, y_train)\n",
    "        X_test_poly = polfeat.transform(X_test)\n",
    "        y_pred = np.clip(reg.predict(X_test_poly),0,None)\n",
    "        predictions.append([[t, r] for t, r in zip(y_pred,y_test[:,0])])\n",
    "    return np.array(predictions)\n",
    "\n",
    "def score_predictions(pred: list) -> int:\n",
    "    '''\n",
    "    Calculate the score (WMAPE) for the predictions given. Input is meant to be same format as 'make_predictions' output.\n",
    "    '''\n",
    "    def wmape(y):\n",
    "        y_pred, y_true = y\n",
    "        if y_true < 1e-2:\n",
    "            return abs(y_true-y_pred)\n",
    "        return abs(y_true-y_pred)/abs(y_true)\n",
    "    return sum(list(map(wmape, pred[:,0])))\n",
    "\n",
    "def make_score_for_city(city_code: str, show_graphs = False) -> int:\n",
    "    dataframe = construct_city_db(city_code)\n",
    "    dataframe = treat_missing(dataframe)\n",
    "    pred = make_predictions(dataframe)\n",
    "    if show_graphs:\n",
    "        plot_regressions(dataframe)\n",
    "    return score_predictions(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the training. This is done using an ElasticNet linear regression, with some basic feature engineering (adding polynomial features). This is useful, in case some of the progression haven't grown linearly with time, but instead quadratically, cubically, or really any polynomial up to a somewhat high degree (15 in this instance). It is worth noting however, ElasticNet enforces both L1 and L2 norms, which means, that the model will only use degree 15 if it is absolutely worth adding, seeing as these norms attempt to reduce the number of used features (which has several advantages, including the reduction of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform to set, just to ensure non-repetition\n",
    "with ThreadPool(12) as p:\n",
    "    a = p.map(make_score_for_city, UNIQUE_CITY_CODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_score_for_city(UNIQUE_CITY_CODES[3], show_graphs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame([[a,b] for a, b in zip(a, unique_city_codes)],columns=['WMAPE', 'city_code']).to_csv('./results/city_clustering_v2.csv')\n",
    "sum(a)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
